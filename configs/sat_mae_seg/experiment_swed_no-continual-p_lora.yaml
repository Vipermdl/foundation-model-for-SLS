task: segmentation
continual_pretrain_run: null
seed: 2
verbose: True
wandb:
  mode: online
  fast_dev_run: False
  entity: dongliang
  project: satmae-seg-swed-no-continual-pretrain-lora
  log_model: True
  experiment_dir: logs/satmae-seg-swed-no-continual-pretrain-lora
  cache_dir: cache/
data:
  datamodule: SWEDDataModule #EuroSATDataModule
  modality: s2
  root: data/swed # data/
  bands: null #[B01, B02, B03, B04, B05, B06, B07, B08, B08A, B09, B11, B12] # [B04,B03,B02]
  num_classes: 1 # 10
  img_size: 224
  few_shot_k: null
  few_shot_seed: null
model:
  name: samseg # 
  backbone: sat_mae
  backbone_type: ""
  loss: "combine"
  feature_map_indices: [5, 11, 17, 23]
  deepsup: True
  patch_size: 16
  freeze_backbone: True
  pretrained: True
  input_res: 10
  fixed_output_size: 0
  embed_dim: 1024
  use_mask_token: True
  adapter: True
  adapter_trainable: True
  norm_trainable: True
  adapter_type: lora
  adapter_scale: 1.0
  adapter_hidden_dim: 8
  adapter_shared: False  # same adapter weights for every attention block
  train_patch_embed: False
  patch_embed_adapter: False
  patch_embed_adapter_scale: 1
  train_cls_mask_tokens: False
  train_all_params: False  # overwrites all the above (freeze_backbone, train_patch_embed, adapter, adapter_shared)
  loss_on_all_patches: True
  only_scaler_trainable: False
  only_bias_trainable: False
  ignore_index: 255
  score_threshold: 0.5
  bands_mean: [560.0944, 669.804, 938.80133, 1104.3877, 1374.6317, 1826.4297, 2012.0166, 2095.8945, 2159.6338, 2191.1506, 2105.7383, 1568.9834]
  bands_std: [175.07619, 236.3873, 268.17673, 328.9421, 326.24823, 404.8634, 447.36502, 486.22122, 464.84232, 450.85526, 413.28418, 369.56287]
knn:
  knn_eval: False
  knn_k: 5
optim:
  lr: 0.000001
  batch_size: 16
  warmup_epochs: 5
  num_workers: 0
  min_steps: 10000
  max_steps: 25000
  aux_loss_factor: 0.1
  lr_schedule_patience: 10
